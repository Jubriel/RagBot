{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16339/3676789303.py:15: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain import hub\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.tools import tool\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOllama(model='qwen2:latest', temperature=0)\n",
    "embed= OllamaEmbeddings(model='mxbai-embed-large:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 4/4 [00:12<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "page_urls = [\"https://hermextravels.com/\",\"https://hermextravels.com/features.html\",\n",
    "            \"https://hermextravels.com/contact.html\", \"https://hermextravels.com/investors.html\"]\n",
    "\n",
    "loader = WebBaseLoader(web_paths=page_urls)\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "# docs = DirectoryLoader('source/', use_multithreading=True).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index from the documents\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "\n",
    "@tool(response_format=\"content\")\n",
    "def retrieve(query:str):\n",
    "    \"\"\"\n",
    "    Retrieve information related to a query\n",
    "\n",
    "    Args:\n",
    "        query (str): user question or comment\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=1)\n",
    "    serialized = \"\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized # , retrieved_docs\n",
    "\n",
    "\n",
    "tools = [retrieve]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve` with `{'query': 'What is Hermex all about?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSource: {'source': 'https://hermextravels.com/investors.html', 'title': 'For Investors — Hermex Travels', 'description': 'Investment opportunities with Hermex Travels', 'language': 'No language found.'}\n",
      "Content: Opportunities for Investors\n",
      "Hermex Travels is at the forefront of travel innovation, integrating AI, AR, and blockchain technology to enhance travel experiences. As we continue to develop, we invite forward-thinking investors to be part of this exciting journey.\n",
      "Interested in helping us bring the future of travel to life?\n",
      "\t\t\t\tContact Us for more details on investment opportunities.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright 2024 Hermex Services Home. All Rights Reserved.\u001b[0m\u001b[32;1m\u001b[1;3mHermex Travels is a company that integrates AI, AR, and blockchain technology to enhance travel experiences. They are inviting forward-thinking investors to be part of their journey towards developing the future of travel. If you're interested in investment opportunities with Hermex Travels, they suggest contacting them for more details.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hermex Travels is a company that integrates AI, AR, and blockchain technology to enhance travel experiences. They are inviting forward-thinking investors to be part of their journey towards developing the future of travel. If you're interested in investment opportunities with Hermex Travels, they suggest contacting them for more details.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({'input':'What is Hermex all about?'})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hera: AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hermex_assistant_chatbot.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model='qwen2:latest', temperature=0)\n",
    "embed = OllamaEmbeddings(model='mxbai-embed-large:latest')\n",
    "\n",
    "page_urls = [\n",
    "            \"https://hermextravels.com/\",\n",
    "            \"https://hermextravels.com/features.html\",\n",
    "            \"https://hermextravels.com/contact.html\",\n",
    "            \"https://hermextravels.com/investors.html\"\n",
    "        ]\n",
    "\n",
    "def setup_vectorstore():\n",
    "    loader = WebBaseLoader(web_paths=page_urls)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vectorstore = FAISS.from_documents(chunks, embed)\n",
    "    return vectorstore\n",
    "\n",
    "retriever = setup_vectorstore()\n",
    "\n",
    "class HermexAssistant:\n",
    "    def __init__(self, streaming:bool=True):\n",
    "        load_dotenv()\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embed\n",
    "        self.prompt_template : ChatPromptTemplate = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are Hera, a professional and efficient human travel assistant for Hermex Travels.\n",
    "            Your purpose is to provide accurate, concise, and helpful answers strictly based on the provided context.\n",
    "            Avoid speculation and only provide facts relevant to Hermex services and the travel/tourism industry.\n",
    "\n",
    "            Use the following context to answer the user's question. If the answer is not in the context, reply with:\n",
    "            \"I'm sorry, I do not have information on that at the moment.\"\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Answer as a knowledgeable, polite, and articulate human travel assistant.You have access to the following tools:\n",
    "            {tools}\n",
    "            Use the following format:\n",
    "\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: the action to take, should be one of [{tool_names}]. Always look first in Retriever\n",
    "            Action Input: the input to the action\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Action Input/Observation can repeat 2 times)\n",
    "            Thought: I now know the final answer\n",
    "            Final Answer: the final answer to the original input question\n",
    "\n",
    "            Begin!\n",
    "\n",
    "            Question: {input}\n",
    "            Thought:{agent_scratchpad}\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.streaming: bool = streaming\n",
    "        self.user_sessions: dict = {}\n",
    "        self.conversational_memory = ConversationBufferWindowMemory(\n",
    "                        memory_key='chat_history',\n",
    "                        k=5,\n",
    "                        return_messages=True)\n",
    "\n",
    "    \n",
    "    @tool(response_format=\"content_and_artifact\")\n",
    "    def retriever(query:str):\n",
    "        \"\"\"\n",
    "        Retrieve information related to a query\n",
    "\n",
    "        Args:\n",
    "            query (str): user question or comment\n",
    "        \"\"\"\n",
    "        retrieved_docs = retriever.similarity_search(query, k=2)\n",
    "        serialized = \"\".join(\n",
    "            (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "            for doc in retrieved_docs\n",
    "        )\n",
    "        return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "    def answer_question(self, user_id:str, query:str)-> str:\n",
    "            return self._standard_answer(user_id, query)\n",
    "\n",
    "    def _standard_answer(self, user_id:str, query:str)-> str:\n",
    "        if user_id not in self.user_sessions:\n",
    "            self.user_sessions[user_id] = []\n",
    "\n",
    "        tools = [retriever]\n",
    "        agent = create_tool_calling_agent(llm, tools, self.prompt_template)\n",
    "\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "        response = agent_executor.invoke({'input':query})\n",
    "        self.user_sessions[user_id].append((query, response['output']))\n",
    "        return response['output']\n",
    "\n",
    "    def get_chat_history(self, user_id:str)-> list:\n",
    "        return self.user_sessions.get(user_id, [])\n",
    "    \n",
    "    def clear_chat_history(self, user_id:str):\n",
    "        if user_id in self.user_sessions:\n",
    "            del self.user_sessions[user_id]\n",
    "            return True\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hera = HermexAssistant(streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "Hera.answer_question(user_id=\"user1\", query=\"What is Hermex all about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hera.answer_question(\"user1\", \"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hera.answer_question(\"user1\", \"who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = \"https://hermex-ai-service.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\"  # Use your target URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferWindowMemory, ChatMessageHistory\n",
    "from langchain_core.prompts import (ChatPromptTemplate, MessagesPlaceholder, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    SystemMessagePromptTemplate)\n",
    "from langchain.agents import tool, create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Models\n",
    "llm = ChatOllama(model='qwen2:latest', temperature=0)\n",
    "embed = OllamaEmbeddings(model='mxbai-embed-large:latest')\n",
    "\n",
    "# FAISS path\n",
    "FAISS_PATH = \"hermex_faiss_index\"\n",
    "\n",
    "# URLs to load\n",
    "page_urls = [\n",
    "    \"https://hermextravels.com/\",\n",
    "    \"https://hermextravels.com/features.html\",\n",
    "    \"https://hermextravels.com/contact.html\",\n",
    "    \"https://hermextravels.com/investors.html\",\n",
    "    \"https://hermextravels.com/investors.html#investment-tiers\"\n",
    "]\n",
    "\n",
    "def setup_vectorstore():\n",
    "    try:\n",
    "        if os.path.exists(FAISS_PATH):\n",
    "            logging.info(\"🔁 Loading FAISS index from disk...\")\n",
    "            return FAISS.load_local(FAISS_PATH, embed, \n",
    "                                    allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            logging.info(\"🔨 Creating FAISS index from webpages...\")\n",
    "\n",
    "            web_loader = WebBaseLoader(web_paths=page_urls)\n",
    "            web_docs = web_loader.load()\n",
    "\n",
    "            text_loader = TextLoader(\"hermex.txt\", encoding=\"utf-8\")\n",
    "            text_docs = text_loader.load()\n",
    "\n",
    "            all_docs = web_docs + text_docs\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                                      chunk_overlap=200, \n",
    "                                                      multithreaded=True)\n",
    "            chunks = splitter.split_documents(all_docs)\n",
    "\n",
    "            vectorstore = FAISS.from_documents(chunks, embed)\n",
    "            vectorstore.save_local(FAISS_PATH)\n",
    "            logging.info(\"✅ FAISS index created and saved.\")\n",
    "            return vectorstore\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up vectorstore: {e}\")\n",
    "        raise RuntimeError(\"Vectorstore setup failed.\") from e\n",
    "\n",
    "\n",
    "class HermexAssistant:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.llm = llm\n",
    "            self.embedding_model = embed\n",
    "            self.user_sessions: dict[str, ChatMessageHistory] = {}\n",
    "            self.vectorstore = setup_vectorstore()\n",
    "\n",
    "            self.instructions = \"\"\"\n",
    "                You are Hera, a professional and efficient travel assistant for \n",
    "                Hermex Travels. Your purpose is to provide accurate, concise, and \n",
    "                helpful answers strictly based on the provided context.\n",
    "                Avoid speculation and only provide facts relevant to Hermex services\n",
    "                and the travel/tourism industry.\n",
    "\n",
    "                Use the following context to answer the user's question. \n",
    "                If the answer is not in the context, reply with:\n",
    "                \"I'm sorry, I can not provide such information at the moment.\"\n",
    "            \"\"\"\n",
    "\n",
    "            prompt = ChatPromptTemplate(\n",
    "                messages=[\n",
    "                    SystemMessagePromptTemplate.from_template(self.instructions),\n",
    "                    MessagesPlaceholder(\"chat_history\"),\n",
    "                    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "                    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Define tools\n",
    "            @tool(response_format=\"content_and_artifact\")\n",
    "            def retrieve():\n",
    "                \"\"\"Retrieve information related to a query to \n",
    "                provide an informative response.\"\"\"\n",
    "                try:\n",
    "                    return self.vectorstore.as_retriever(search_kwargs={\"k\": 2}), \\\n",
    "                            self.conversational_memory.chat_memory\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error during retrieval: {e}\")\n",
    "                    return \"Retrieval failed.\"\n",
    "\n",
    "            @tool(response_format=\"content\")\n",
    "            def ip_address():\n",
    "                \"\"\"Retrieve IP-address information of the user\"\"\"\n",
    "                try:\n",
    "                    from requests import get\n",
    "                    return get('https://api.ipify.org').text\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error fetching IP address: {e}\")\n",
    "                    return \"Could not retrieve IP address.\"\n",
    "\n",
    "            @tool(response_format=\"content\")\n",
    "            def date_time():\n",
    "                \"\"\"Retrieve current day, date, and time in a readable format\"\"\"\n",
    "                try:\n",
    "                    from datetime import datetime\n",
    "                    now = datetime.now()\n",
    "                    return now.strftime(\"%A, %Y-%m-%d %H:%M:%S\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error getting date and time: {e}\")\n",
    "                    return \"Could not retrieve date/time.\"\n",
    "\n",
    "            self.conversational_memory = ConversationBufferWindowMemory(\n",
    "                memory_key='chat_history',\n",
    "                k=5,\n",
    "                return_messages=True\n",
    "            )\n",
    "\n",
    "            self.agent_executor = AgentExecutor(\n",
    "                agent=create_tool_calling_agent(self.llm, [retrieve, \n",
    "                                                           date_time, \n",
    "                                                           ip_address], prompt),\n",
    "                tools=[retrieve, date_time, ip_address],\n",
    "                memory=self.conversational_memory,\n",
    "                # verbose=True,\n",
    "            )\n",
    "            logging.info(\"🤖 HermexAssistant initialized successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to initialize HermexAssistant: {e}\")\n",
    "            raise\n",
    "\n",
    "    def chat(self, user_id: str, query: str) -> str:\n",
    "        \"\"\"Handles chat with a specific user session.\"\"\"\n",
    "        try:\n",
    "            if user_id not in self.user_sessions:\n",
    "                self.user_sessions[user_id] = ChatMessageHistory()\n",
    "\n",
    "            self.conversational_memory.chat_memory = self.user_sessions[user_id]\n",
    "            self.user_sessions[user_id].add_user_message(query)\n",
    "\n",
    "            response = self.agent_executor.invoke({'input': query})\n",
    "            output = response.get('output', \n",
    "                                  \"I'm sorry, I couldn't generate a response.\")\n",
    "            self.user_sessions[user_id].add_ai_message(output)\n",
    "            return output\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Chat error for user {user_id}: {e}\")\n",
    "            return \"Something went wrong. Please try again later.\"\n",
    "\n",
    "    def load_history(self, user_id: str) -> list:\n",
    "        \"\"\"Returns user's conversation history or initializes it if missing.\"\"\"\n",
    "        try:\n",
    "            if user_id not in self.user_sessions:\n",
    "                self.user_sessions[user_id] = ChatMessageHistory()\n",
    "            return self.user_sessions[user_id].messages\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading history for user {user_id}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clear_chat_history(self, user_id: str):\n",
    "        try:\n",
    "            if user_id in self.user_sessions:\n",
    "                del self.user_sessions[user_id]\n",
    "                logging.info(f\"Cleared chat history for user {user_id}\")\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clearing chat history for user {user_id}: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Loading FAISS index from disk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hermex Travels is a professional travel assistance service dedicated to providing accurate, concise, and helpful answers based on the provided context. Our focus is on offering information strictly related to Hermex services and the travel/tourism industry without speculation or irrelevant details.\\n\\nTo provide you with more specific information about Hermex Travels, I would need additional data or context about the aspects of Hermex you are interested in learning about. Please specify if you want to know about their services, destinations they offer, booking process, customer support, or any other relevant topic related to travel assistance and tourism.\\n\\nIf you're looking for general information on how we operate as a professional service provider within the travel industry, I can confirm that our primary goal is to ensure smooth and efficient travel experiences for our clients by providing them with accurate information and assistance tailored to their needs.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "her = HermexAssistant(streaming=False)\n",
    "her.chat(user_id=\"user1\", query=\"What is Hermex all about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Hera, a professional and efficient travel assistant designed to provide accurate, concise, and helpful answers based on the provided context related to Hermex Travels services and the travel/tourism industry. My purpose is to assist with inquiries about destinations, booking processes, customer support, and other relevant information without speculation or irrelevant details.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "her.chat(user_id=\"user1\", query=\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your IP address is 102.89.47.73.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "her.chat(user_id=\"user1\", query=\"what is my ip?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your last question was: \"What is my ip?\"'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "her.chat(user_id=\"user1\", query=\"what was my last question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is 2023-11-28.\\n\\nToday's date is November 28, 2023.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "her.chat(user_id=\"user1\", query=\"what is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "google/flan-t5-xxl is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/flan-t5-xxl/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67fc203c-060ac5ef44fc57e53a3b8bcf;abb56103-5a86-4342-9a10-8d84165558f6)\n\nRepository Not Found for url: https://huggingface.co/google/flan-t5-xxl/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid credentials in Authorization header",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m load_dotenv()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Set up LLM using Hugging Face Hub\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-xxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_kwargs={\"temperature\": 0.2, \"max_length\": 512}\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Set up embeddings\u001b[39;00m\n\u001b[1;32m     23\u001b[0m embed \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:127\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`device_map` is already specified in `model_kwargs`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m     _model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device_map\n\u001b[0;32m--> 127\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:871\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    873\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:703\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    702\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 703\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: google/flan-t5-xxl is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferWindowMemory, ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.agents import tool, create_tool_calling_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up LLM using Hugging Face Hub\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-xxl\", \n",
    "    task = 'text generation'    # model_kwargs={\"temperature\": 0.2, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "# Set up embeddings\n",
    "embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "FAISS_PATH = \"hermex_faiss_index\"\n",
    "\n",
    "page_urls = [\n",
    "    \"https://hermextravels.com/\",\n",
    "    \"https://hermextravels.com/features.html\",\n",
    "    \"https://hermextravels.com/contact.html\",\n",
    "    \"https://hermextravels.com/investors.html\",\n",
    "    \"https://hermextravels.com/investors.html#investment-tiers\"\n",
    "]\n",
    "\n",
    "def setup_vectorstore():\n",
    "    if os.path.exists(FAISS_PATH):\n",
    "        print(\"🔁 Loading FAISS index from disk...\")\n",
    "        return FAISS.load_local(FAISS_PATH, embed,\n",
    "                                allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(\"🔨 Creating FAISS index from webpages...\")\n",
    "        web_loader = WebBaseLoader(web_paths=page_urls)\n",
    "        web_docs = web_loader.load()\n",
    "\n",
    "        text_loader = TextLoader(\"hermex.txt\", encoding=\"utf-8\")\n",
    "        text_docs = text_loader.load()\n",
    "\n",
    "        all_docs = web_docs + text_docs\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                                  chunk_overlap=200, \n",
    "                                                  multithreaded=True)\n",
    "        chunks = splitter.split_documents(all_docs)\n",
    "        vectorstore = FAISS.from_documents(chunks, embed)\n",
    "        vectorstore.save_local(FAISS_PATH)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "class HermexAssistant:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embed\n",
    "        self.user_sessions: dict[str, ChatMessageHistory] = {}\n",
    "        self.vectorstore = setup_vectorstore()\n",
    "\n",
    "        self.instructions =  \"\"\"\n",
    "            You are Hera, a professional and efficient travel assistant for Hermex Travels.\n",
    "            Your purpose is to provide accurate, concise, and helpful answers strictly based on the provided context.\n",
    "            Avoid speculation and only provide facts relevant to Hermex services and the travel/tourism industry.\n",
    "\n",
    "            Use the following context to answer the user's question. If the answer is not in the context, reply with:\n",
    "            \"I'm sorry, I can not provide such information at the moment.\"\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate(\n",
    "            messages=[\n",
    "                SystemMessagePromptTemplate.from_template(self.instructions),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "                MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        @tool(response_format=\"content_and_artifact\")\n",
    "        def retrieve() -> str:\n",
    "            \"\"\"\n",
    "            Retrieve information related to a query to provide an informative response.\n",
    "            \"\"\"\n",
    "            retrieved = self.vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "            return retrieved, self.conversational_memory.chat_memory\n",
    "\n",
    "        @tool(response_format=\"content\")\n",
    "        def ip_address():\n",
    "            \"\"\"\n",
    "            Retrieve IP-address information of the user\n",
    "            \"\"\"\n",
    "            from requests import get\n",
    "            return get('https://api.ipify.org').text\n",
    "\n",
    "        @tool(response_format=\"content\")\n",
    "        def date_time():\n",
    "            \"\"\"\n",
    "            Retrieve current day, date, and time in a readable format\n",
    "            \"\"\"\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            return now.strftime(\"%A, %Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        self.conversational_memory = ConversationBufferWindowMemory(\n",
    "            memory_key='chat_history',\n",
    "            k=5,\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=create_tool_calling_agent(self.llm, [retrieve, date_time, ip_address], prompt),\n",
    "            tools=[retrieve, date_time, ip_address],\n",
    "            memory=self.conversational_memory,\n",
    "        )\n",
    "\n",
    "    def chat(self, user_id: str, query: str) -> str:\n",
    "        if user_id not in self.user_sessions:\n",
    "            self.user_sessions[user_id] = ChatMessageHistory()\n",
    "\n",
    "        self.conversational_memory.chat_memory = self.user_sessions[user_id]\n",
    "\n",
    "        self.user_sessions[user_id].add_user_message(query)\n",
    "        response = self.agent_executor.invoke({'input': query})\n",
    "        self.user_sessions[user_id].add_ai_message(response['output'])\n",
    "        return response['output']\n",
    "\n",
    "    def load_history(self, user_id: str) -> list:\n",
    "        return self.user_sessions.get(user_id, ChatMessageHistory())\n",
    "\n",
    "    def clear_chat_history(self, user_id: str):\n",
    "        if user_id in self.user_sessions:\n",
    "            del self.user_sessions[user_id]\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
