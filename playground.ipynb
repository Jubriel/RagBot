{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31942/1986130783.py:15: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain import hub\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.tools import tool\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOllama(model='qwen2:latest', temperature=0)\n",
    "embed= OllamaEmbeddings(model='mxbai-embed-large:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "docs = DirectoryLoader('source/', use_multithreading=True).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # index = faiss.IndexFlatL2(len(embed.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embed,\n",
    ")\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "db = FAISS.load_local(\"faiss_index\", embed, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-10 09:57:55.664\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m429\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: 401 Client Error. (Request ID: Root=1-67f78813-4b8e6cb81786acc0392fc93a;2b5b7f6b-f72c-495e-b8cb-86103ca6c63b)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/qdrant/bge-small-en-v1.5-onnx-q.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid credentials in Authorization header Falling back to other sources.\u001b[0m\n",
      "\u001b[32m2025-04-10 09:57:55.665\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m450\u001b[0m - \u001b[31m\u001b[1mCould not download model from either source, sleeping for 3.0 seconds, 2 retries left.\u001b[0m\n",
      "\u001b[32m2025-04-10 09:57:58.940\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m429\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: 401 Client Error. (Request ID: Root=1-67f78816-0930b6826aa2f0437e77d6a8;fa5db851-e334-4cb1-9dc6-798652496faf)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/qdrant/bge-small-en-v1.5-onnx-q.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid credentials in Authorization header Falling back to other sources.\u001b[0m\n",
      "\u001b[32m2025-04-10 09:57:58.941\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m450\u001b[0m - \u001b[31m\u001b[1mCould not download model from either source, sleeping for 9.0 seconds, 1 retries left.\u001b[0m\n",
      "\u001b[32m2025-04-10 09:58:08.291\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m429\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: 401 Client Error. (Request ID: Root=1-67f78820-14d8c05831930d88792fc753;d6737a18-3a5c-483d-a267-3e5817450d98)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/qdrant/bge-small-en-v1.5-onnx-q.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid credentials in Authorization header Falling back to other sources.\u001b[0m\n",
      "\u001b[32m2025-04-10 09:58:08.293\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m450\u001b[0m - \u001b[31m\u001b[1mCould not download model from either source, sleeping for 27.0 seconds, 0 retries left.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for FastEmbedEmbeddings\n  Value error, Could not load model BAAI/bge-small-en-v1.5 from any source. [type=value_error, input_value={'model_name': 'BAAI/bge-...s': None, 'model': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastembed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastEmbedEmbeddings\n\u001b[0;32m----> 2\u001b[0m embed_2 \u001b[38;5;241m=\u001b[39m \u001b[43mFastEmbedEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      5\u001b[0m     documents\u001b[38;5;241m=\u001b[39msplits,\n\u001b[1;32m      6\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membed_2,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for FastEmbedEmbeddings\n  Value error, Could not load model BAAI/bge-small-en-v1.5 from any source. [type=value_error, input_value={'model_name': 'BAAI/bge-...s': None, 'model': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "# embed_2 = FastEmbedEmbeddings()\n",
    "\n",
    "# vector_store = FAISS.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embed_2,\n",
    "# )\n",
    "\n",
    "# vector_store.save_local(\"faiss_index\")\n",
    "# db = FAISS.load_local(\"faiss_index\", embed, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query:str):\n",
    "    \"\"\"\n",
    "    Retrieve information related to a query\n",
    "\n",
    "    Args:\n",
    "        query (str): user question or comment\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=1)\n",
    "    serialized = \"\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "@tool(response_format=\"content\")\n",
    "def ip_address():\n",
    "    \"\"\"\n",
    "    Retrieve IP address information of the user\n",
    "    \"\"\"\n",
    "    from requests import get\n",
    "\n",
    "    ip = get('https://api.ipify.org').text\n",
    "    return ip\n",
    "\n",
    "tools = [retrieve, ip_address]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({'input':'What is the full concept of MLOps?'})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({'input':'what is my ip address?'})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agentic():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
